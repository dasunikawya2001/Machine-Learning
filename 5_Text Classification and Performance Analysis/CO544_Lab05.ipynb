{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Lab 05: Text Classification and Performance Analysis**"
      ],
      "metadata": {
        "id": "S415NsDFzDUt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Classification"
      ],
      "metadata": {
        "id": "Jt9uoOVazLn3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing required modules"
      ],
      "metadata": {
        "id": "L3Wi9CRR7VjV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Regular expressions*\n",
        "\n",
        "* re is a Python module for regular expressions.\n",
        "\n",
        "* It allows pattern matching in strings. For example, remove punctuation, match words\n",
        "\n",
        "* If the inout \"This! is, some# text.\", then the output will be \"This  is  some  text \".\n",
        "\n",
        "*from sklearn.datasets import load_files*\n",
        "\n",
        "* This function loads a dataset from folders.\n",
        "* It automatically assigns labels based on folder names.\n",
        "\n",
        "*from nltk.corpus import stopwords*\n",
        "\n",
        "* stopwords are common words like the, is, in, on, etc.\n",
        "* These words don't carry meaningful sentiment, so we often remove them during text preprocessing.\n",
        "\n",
        "*from nltk.stem import WordNetLemmatizer*\n",
        "\n",
        "* Lemmatization = converting a word to its base form (dictionary form).\n",
        "* Example: \"running\" → \"run\", \"better\" → \"good\"\n",
        "\n"
      ],
      "metadata": {
        "id": "mvoPVCovzSFN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re # Regular expressions\n",
        "from sklearn.datasets import load_files # For loading dataset folders\n",
        "import nltk # Natural Language Toolkit\n",
        "\n",
        "from nltk.corpus import stopwords # Stop words\n",
        "from nltk.stem import WordNetLemmatizer # Lemmatization\n",
        "\n",
        "# Download required NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGBAPuT3zRgE",
        "outputId": "303f5c66-3a2e-4d64-a21f-60c3bd33d89f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading data"
      ],
      "metadata": {
        "id": "pInRWGxq7P1k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCZiDHd_snGq",
        "outputId": "086b7d7a-c62d-427f-c09b-b2f170af5cba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "# Instantiate lemmatizer (needed for later)\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# movie_data = load_files(r\"txt_sentoken\")\n",
        "movie_data = load_files('/content/drive/My Drive/CO544/movie_reviews/')\n",
        "X, y = movie_data.data, movie_data.target"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Show basic summary information\n",
        "print(f\"Number of documents: {len(X)}\")\n",
        "print(f\"Number of labels: {len(y)}\")\n",
        "print(f\"Target names (classes): {movie_data.target_names}\")\n",
        "\n",
        "# Show a sample file (before decoding)\n",
        "print(\"\\nFirst document (raw bytes):\")\n",
        "print(X[0][:500]) # show first 500 bytes\n",
        "\n",
        "# Decode and print a preview\n",
        "print(\"\\nFirst document (decoded):\")\n",
        "print(X[0].decode('utf-8')[:500]) # show first 500 characters\n",
        "\n",
        "# Check label of first document\n",
        "print(f\"\\nLabel of first document: {y[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCBzOo3r-udu",
        "outputId": "504af109-1129-4ec5-b5e2-c1dc5f393780"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of documents: 2017\n",
            "Number of labels: 2017\n",
            "Target names (classes): ['neg', 'pos']\n",
            "\n",
            "First document (raw bytes):\n",
            "b\"my opinion on a film can be easily swayed by the presence of actors i love . \\ni love ralph fiennes . \\ni love uma thurman . \\ni love sean connery . \\nhell , i'm even a big fan of jim broadbent and fiona shaw . \\ni saw the fantastic preview for the avengers nearly eight months ago , and i've been eagerly awaiting the film ever since . \\na few months into the summer , however , i noticed that its release date had been changed a few times , and that it had ended up in the mid-august dumping ground . \\nth\"\n",
            "\n",
            "First document (decoded):\n",
            "my opinion on a film can be easily swayed by the presence of actors i love . \n",
            "i love ralph fiennes . \n",
            "i love uma thurman . \n",
            "i love sean connery . \n",
            "hell , i'm even a big fan of jim broadbent and fiona shaw . \n",
            "i saw the fantastic preview for the avengers nearly eight months ago , and i've been eagerly awaiting the film ever since . \n",
            "a few months into the summer , however , i noticed that its release date had been changed a few times , and that it had ended up in the mid-august dumping ground . \n",
            "th\n",
            "\n",
            "Label of first document: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Datapreprocessing"
      ],
      "metadata": {
        "id": "reVJN787_Xac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents = []\n",
        "\n",
        "for i in range(len(X)):\n",
        "  # 1. Decode from bytes to string\n",
        "  document = X[i].decode('utf-8')\n",
        "  # # you can add a small check as follows.\n",
        "  # print(X[0]) # Before decoding (bytes)\n",
        "  # print(X[0].decode('utf-8')) # After decoding\n",
        "\n",
        "  # 2. Apply your regex substitutions\n",
        "  document = re.sub(r'\\W', ' ', document) # remove special characters\n",
        "  document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) # single chars at beginning\n",
        "  document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document) # single chars in middle\n",
        "  document = re.sub(r'\\d+', ' ', document) # remove numbers\n",
        "  document = re.sub(r'\\s+', ' ', document, flags=re.I) # multiple spaces to one\n",
        "\n",
        "  # 3. Lowercase\n",
        "  document = document.lower()\n",
        "\n",
        "  # 4. Tokenize\n",
        "  document = document.split()\n",
        "\n",
        "  # 5. Lemmatize\n",
        "  document = [lemmatizer.lemmatize(word) for word in document]\n",
        "\n",
        "  # 6. Rejoin tokens if needed (optional)\n",
        "  document = ' '.join(document)\n",
        "\n",
        "  # 7. Append to new list\n",
        "  documents.append(document)"
      ],
      "metadata": {
        "id": "1-WbJU5u_cbt"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convert text intonumbers"
      ],
      "metadata": {
        "id": "Z-yKeoTSEUVj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "vectorizer = CountVectorizer(\n",
        "  max_features=1500,\n",
        "  min_df=7,\n",
        "  max_df=0.8,\n",
        "  stop_words=stopwords.words('english')\n",
        ")\n",
        "\n",
        "X_vectors = vectorizer.fit_transform(documents).toarray()\n",
        "\n",
        "# To check the shape and vocabulary:\n",
        "print(X_vectors.shape) # (number_of_documents, number_of_features)\n",
        "print(vectorizer.get_feature_names_out()) # List of feature words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvMvHVAOEVST",
        "outputId": "81d2d0c3-2256-4b13-9773-1ecf4adcacc9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2017, 1500)\n",
            "['ability' 'able' 'absolutely' ... 'york' 'young' 'younger']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text Classification"
      ],
      "metadata": {
        "id": "wydDxVDzK5po"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_vectors, y, test_size=0.2,\n",
        "random_state=0)\n",
        "\n",
        "# Logistic Regression model\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "log_reg = LogisticRegression()\n",
        "\n",
        "# Train the model\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "# Make the predictions on testing data\n",
        "predictions = log_reg.predict(X_test)"
      ],
      "metadata": {
        "id": "3mc5K0y8LAyw"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluating Model Performance"
      ],
      "metadata": {
        "id": "xsoxhIh_L-Md"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, predictions))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, predictions))\n",
        "print(\"\\nAccuracy:\")\n",
        "print(accuracy_score(y_test, predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lNYgn4WL_A9",
        "outputId": "b1c5cb3f-7d38-40d5-9ced-38f18db364e0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "[[152  42]\n",
            " [ 41 169]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.78      0.79       194\n",
            "           1       0.80      0.80      0.80       210\n",
            "\n",
            "    accuracy                           0.79       404\n",
            "   macro avg       0.79      0.79      0.79       404\n",
            "weighted avg       0.79      0.79      0.79       404\n",
            "\n",
            "\n",
            "Accuracy:\n",
            "0.7945544554455446\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train three additional classifiers"
      ],
      "metadata": {
        "id": "wxDXAsnxN0PZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Required Models"
      ],
      "metadata": {
        "id": "KvPQ25wnN50A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
      ],
      "metadata": {
        "id": "mCfZd8YpN0wu"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train All Models"
      ],
      "metadata": {
        "id": "uuA2yHjoN-XJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression (already trained before)\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "log_reg = LogisticRegression()\n",
        "log_reg.fit(X_train, y_train)\n",
        "log_pred = log_reg.predict(X_test)\n",
        "\n",
        "# Random Forest\n",
        "rf = RandomForestClassifier(random_state=0)\n",
        "rf.fit(X_train, y_train)\n",
        "rf_pred = rf.predict(X_test)\n",
        "\n",
        "# SVM\n",
        "svm = SVC(kernel='linear')  # Linear kernel for text data\n",
        "svm.fit(X_train, y_train)\n",
        "svm_pred = svm.predict(X_test)\n",
        "\n",
        "# Naive Bayes\n",
        "nb = MultinomialNB()\n",
        "nb.fit(X_train, y_train)\n",
        "nb_pred = nb.predict(X_test)\n"
      ],
      "metadata": {
        "id": "isy6M8ATN_t0"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate All Models"
      ],
      "metadata": {
        "id": "KQ8mzzbeODFu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(name, y_test, y_pred):\n",
        "    print(f\"--- {name} ---\")\n",
        "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "\n",
        "evaluate_model(\"Logistic Regression\", y_test, log_pred)\n",
        "evaluate_model(\"Random Forest\", y_test, rf_pred)\n",
        "evaluate_model(\"Support Vector Machine\", y_test, svm_pred)\n",
        "evaluate_model(\"Naive Bayes\", y_test, nb_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LiVBrwb1OF2j",
        "outputId": "9201e691-9971-4be9-8064-c035e73da61d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Logistic Regression ---\n",
            "Accuracy: 0.7945544554455446\n",
            "Confusion Matrix:\n",
            " [[152  42]\n",
            " [ 41 169]]\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.78      0.79       194\n",
            "           1       0.80      0.80      0.80       210\n",
            "\n",
            "    accuracy                           0.79       404\n",
            "   macro avg       0.79      0.79      0.79       404\n",
            "weighted avg       0.79      0.79      0.79       404\n",
            "\n",
            "--- Random Forest ---\n",
            "Accuracy: 0.8193069306930693\n",
            "Confusion Matrix:\n",
            " [[164  30]\n",
            " [ 43 167]]\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.85      0.82       194\n",
            "           1       0.85      0.80      0.82       210\n",
            "\n",
            "    accuracy                           0.82       404\n",
            "   macro avg       0.82      0.82      0.82       404\n",
            "weighted avg       0.82      0.82      0.82       404\n",
            "\n",
            "--- Support Vector Machine ---\n",
            "Accuracy: 0.754950495049505\n",
            "Confusion Matrix:\n",
            " [[150  44]\n",
            " [ 55 155]]\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      0.77      0.75       194\n",
            "           1       0.78      0.74      0.76       210\n",
            "\n",
            "    accuracy                           0.75       404\n",
            "   macro avg       0.76      0.76      0.75       404\n",
            "weighted avg       0.76      0.75      0.76       404\n",
            "\n",
            "--- Naive Bayes ---\n",
            "Accuracy: 0.7970297029702971\n",
            "Confusion Matrix:\n",
            " [[154  40]\n",
            " [ 42 168]]\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.79      0.79       194\n",
            "           1       0.81      0.80      0.80       210\n",
            "\n",
            "    accuracy                           0.80       404\n",
            "   macro avg       0.80      0.80      0.80       404\n",
            "weighted avg       0.80      0.80      0.80       404\n",
            "\n"
          ]
        }
      ]
    }
  ]
}